---
title: 'Class 7: Multi-layer hierarchical models'
author: Andrew Parnell \newline \texttt{andrew.parnell@mu.ie}   \newline \vspace{1cm}
  \newline \includegraphics[width=3cm]{maynooth_uni_logo.jpg}
output:
  beamer_presentation:
    includes:
      in_header: header.tex
classoption: "aspectratio=169"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = 'pdf')
knitr::opts_knit$set(global.par = TRUE)
set.seed(123)
library(R2jags)
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,las=1)
```

## Learning outcomes:

- Understand how to add in multiple layers to a hierarchical model
- Follow a detailed example of building a model
- Be able to work with missing data in JAGS

## Some new terminology

- Most of the models we have covered so far contain only one hidden or _latent_ set of parameters
- For example, the data $y$ may depend on a parameter $\beta$, which itself depends on a parameter $\theta$. $\theta$ is given a prior distribution
- We say that the data are at the 'top level', the parameter $\beta$ is a _latent parameter_ at the second level, and the hyper-parameter $\theta$ is also a latent parameter at the third level
- We say that the prior distribution on $\beta$ is _conditional_ on $\theta$, whilst the prior distribution (if it just involves numbers) is a _marginal prior distribution_

## What is a multi-layer model?

- A multi-layer model is one where have many (usually more than 2 or 3)  layers of parameters conditional on each other
- It's very straightforward to add in these extra layers in JAGS/Stan
- The question is whether they are necessary or not, and how much the data can tell us about them

## Writing the same model in different ways

- The model on the previous slides has a different intercept and slope for each ethnicity group, with the information about them tied together through the prior distributions on them
- The likelihood was written as:
```
    y[i] ~ dnorm(alpha[eth[i]] + 
                  beta[eth[i]]*(x[i] - mean(x)), 
                    sigma^-2)
```
which in maths can be written as:
$$y_i \sim N(\alpha_{\mbox{eth}_i} + \beta_{\mbox{eth}_i} x_i, \sigma^2)$$
where $\mbox{eth}_i$ takes the values 1, 2, 3, or 4

- Remember, $y_i$ is the log-earnings of individual $i$ where $i=1,\ldots,N$

## Re-writing the model 

- Commonly you'll see $y$ here re-defined as $y_{ij}$ where $j=1,..,4$ represents ethnicity, and $i=1,\ldots,N_j$ is the number of individuals with ethnicity $j$
- The likelihood can then be written as:
$$y_{ij} \sim N(\alpha_j + \beta_j x_{ij}, \sigma^2)$$
- Note that this is exactly the same model, just re-written slightly differently. In fact, this latter model is much harder to write out in JAGS/Stan code

## Fixed vs random effect models

\small

- Thinking about this model in more detail
$$y_{ij} \sim N(\alpha_j + \beta_j x_{ij}, \sigma^2)$$
- If the $\alpha_j$ and $\beta_j$ parameters are all given independent prior distributions, e.g. $\alpha_j \sim N(0, 100)$ or similar, then this is considered a _fixed effects_ model
- If the $\alpha_j$ and $\beta_j$ are given prior distributions that tie the values together, e.g. $\alpha_j \sim N(\mu_\alpha, \sigma_\alpha^2)$, then this is often called a _random effects_ model
- (In fact, nobody can agree on what a fixed or random effects model actually is)

## Mixed effects vs hierarchical models

- The hierarchical models we have been studying all use the _random effects_ approach wherever possible
- The big advantage of using this approach is that we get to _borrow_ strength between the different groups (here `eth`, but it could be anything)
- Whenever we have a categorical covariate we should always be putting a constraining/tying prior distribution on them, and looking at how the effects vary between the groups
- Mathematically you can write out the hierarchicaly estimated intercepts of a group ($\alpha_j$) as a weighted average of the group intercept means from the data ($\bar{\alpha}_j$) and the overall mean of the entire data set ($\mu$) where the weights are dependent on the group and overall variance and sample sizes. 
- Because of the weighted nature of the estimate this is often called _partial pooling_ or _shrinkage_

## Example: multi-layer earnings data

- We will now go through and build a much more complicated model for the earnings data, taken from the Gelman and Hill book, using only weak priors
- We can generate data from these models (either using the prior or the posterior)
- Our goal is to explore the factors which explain earnings. We have variables on height, age, and ethnicity.
- If we first plot the data
```{r, echo = FALSE, fig.height=3}
dat = read.csv('../data/earnings.csv')
par(mfrow=c(1,2))
plot(jitter(dat$height_cm), dat$earn, xlab = 'Height (cm)', ylab = 'Earnings ($)')
plot(jitter(dat$height_cm), log(dat$earn), xlab = 'Height (cm)', ylab = 'log(Earnings ($))')
par(mfrow=c(1,1))
```

## Transformations

- From the left-hand plot there seem to be quite a few extreme observations, and there's a possibility that the relationship between height and earnings is non-linear
- The right-hand plot seems to have stabilised most of the extreme observations, and perhaps linearity is more appropriate
- Notice that a linear model implies:
$$y_i = \alpha + \beta x_i + \epsilon_i$$
whilst the log-linear model implies:
$$y_i = \exp( \alpha + \beta x_i + \epsilon_i) = e^\alpha \times e^{\beta x_i} \times e^\epsilon_i$$
so the coefficients, once exponentiated, have multiplicative effects
that are relatively easy to interpret

## Fitting the first model

- If we fit a model with just height (mean centered) we get the following JAGS output

\tiny
```{r, include = FALSE}
jags_code = '
model{
  # Likelihood
  for(i in 1:N) {
    log_earn[i] ~ dnorm(alpha + beta_height*(height[i] - mean(height)), 
                    sigma^-2)
  }
  # Priors
  alpha ~ dnorm(11, 2^-2)
  beta_height ~ dnorm(0, 0.1^-2)
  sigma ~ dt(0, 5^-2, 1)T(0,)
}
'
jags_run = jags(data = list(N = nrow(dat), 
                            log_earn = log(dat$earn),
                            height = dat$height_cm),
                parameters.to.save = c('alpha',
                                       'beta_height',
                                       'sigma'),
                model.file = textConnection(jags_code))
```
```{r, echo = FALSE}
print(jags_run)
```

## Interpreting the parameters

- These parameters are directly interpretable:

```{r, include = FALSE}
jags_summ = jags_run$BUGSoutput$summary
low = signif(exp(jags_summ['alpha','2.5%']),2)
high = signif(exp(jags_summ['alpha','97.5%']),2)
sig = jags_summ['sigma','mean']
beta = jags_summ['beta_height','mean']
DIC = round(jags_run$BUGSoutput$DIC,2)
pD = round(jags_run$BUGSoutput$pD,2)
```

    - The mean of the log earnings at the mean height is about 9.737, which is about 17k on the original scale
    - We can also get e.g. a 95% confidence interval using the JAGS output. From $`r format(low, scientific = FALSE)`$ to $`r format(high, scientific = FALSE)`$
    - For every extra cm so you gain $`r format(beta, scientific = FALSE, digits = 3)`$ on the log scale, i.e. an $`r format(100*exp(beta)-100, scientific = FALSE, digits = 3)`$% gain in income
    - From the posterior of $\sigma$, we can guess that about 68% of predictions will be within $`r format(sig, scientic = FALSE, digits = 3)`$ on the log scale or within a factor of about $`r format(exp(sig), scientic = FALSE, digits = 3)`$ of the prediction 
- Interpretation for the intercept would have been harder had we not mean-centered the height variable
- The DIC is $`r format(DIC,scientific=FALSE)`$ with $`r format(pD,scientific=FALSE)`$ effective parameters

## Improving the model

- Now suppose we fit a model with a random intercept for ethnicity

\small
```{r}
jags_code = '
model{
  # Likelihood
  for(i in 1:N) {
    log_earn[i] ~ dnorm(alpha_eth[eth[i]] + 
                    beta_height*(height[i] - mean(height)), 
                    sigma^-2)
  }
  # Priors
  for(j in 1:N_eth) {
    alpha_eth[j] ~ dnorm(mu_eth, sigma_eth^-2)
  }
  beta_height ~ dnorm(0, 0.1^-2)
  mu_eth ~ dnorm(11, 2^-2)
  sigma_eth ~ dt(0, 5^-2, 1)T(0,)
  sigma ~ dt(0, 5^-2, 1)T(0,)
}
'
```

## Improving the model 2

```{r, message = FALSE, results = 'hide', echo = FALSE}
jags_run = jags(data = list(N = nrow(dat), 
                            N_eth = length(unique(dat$eth)),
                            log_earn = log(dat$earn),
                            height = dat$height_cm,
                            eth = dat$eth),
                parameters.to.save = c('alpha_eth',
                                       'beta_height',
                                       'mu_eth',
                                       'sigma'),
                model.file = textConnection(jags_code))
```

\tiny
```{r, echo = FALSE}
print(jags_run)
```

## Interpreting the output

```{r, include = FALSE}
jags_summ = jags_run$BUGSoutput$summary
low = signif(exp(jags_summ['mu_eth','2.5%']),2)
high = signif(exp(jags_summ['mu_eth','97.5%']),2)
DIC = round(jags_run$BUGSoutput$DIC,2)
pD = round(jags_run$BUGSoutput$pD,2)
```

- The parameters $\alpha$ and $\beta_{\mbox{height}}$ haven't changed much in the mean 
- The 95% confidence interval for $\alpha$ has increased: $`r format(low, scientific = FALSE)`$ to $`r format(high, scientific = FALSE)`$
- The DIC is $`r format(DIC,scientific=FALSE)`$ with $`r format(pD,scientific=FALSE)`$ effective parameters. Pretty much the same as above
- We also have estimates for each ethnicity, none of these have a strong effect away from zero

## Now an interaction model

\tiny
```{r}
jags_code = '
model{
  # Likelihood
  for(i in 1:N) {
    log_earn[i] ~ dnorm(alpha_eth[eth[i]] + 
                    beta_height[eth[i]]*(height[i] - mean(height)), 
                    sigma^-2)
  }
  # Priors
  for(j in 1:N_eth) {
    alpha_eth[j] ~ dnorm(mu_eth, sigma_eth^-2)
    beta_height[j] ~ dnorm(mu_beta_height, sigma_height^-2)
  }
  mu_beta_height ~ dnorm(0, 0.1^-2)
  mu_eth ~ dnorm(11, 2^-2)
  sigma_eth ~ dt(0, 5^-2 ,1)T(0,)
  sigma_height ~ dt(0, 1, 1)T(0,)
  sigma ~ dt(0, 5^-2, 1)T(0,)
}
'
```

## Interaction model results


```{r, include = FALSE}
jags_run = jags(data = list(N = nrow(dat), 
                            N_eth = length(unique(dat$eth)),
                            log_earn = log(dat$earn),
                            height = dat$height_cm,
                            eth = dat$eth),
                parameters.to.save = c('alpha_eth',
                                       'beta_height',
                                       'beta_eth',
                                       'mu_eth',
                                       'mu_beta_height',
                                       'sigma_height',
                                       'sigma'),
                model.file = textConnection(jags_code))
```

\tiny
```{r, echo = FALSE}
print(jags_run)
```

## Interpreting the output

```{r, include = FALSE}
jags_summ = jags_run$BUGSoutput$summary
low = signif(exp(jags_summ['mu_eth','2.5%']),2)
high = signif(exp(jags_summ['mu_eth','97.5%']),2)
DIC = round(jags_run$BUGSoutput$DIC,2)
pD = round(jags_run$BUGSoutput$pD,2)
```

- The model has improved a bit, DIC now $`r format(DIC,scientific=FALSE)`$ with $`r format(pD,scientific=FALSE)`$ effective parameters
- The confidence intervals for the different slopes are highly different, with the whites group (ethnicity = 3) having a much clearer relationship with height, possibly due to the large sample size
- Go back to the previous classes to see plots of these effects

## Checking the model - posterior predictive fit

```{r, include = FALSE}
jags_code = '
model{
  # Likelihood
  for(i in 1:N) {
    log_earn[i] ~ dnorm(alpha_eth[eth[i]] + 
                    beta_height[eth[i]]*(height[i] - mean(height)), 
                    sigma^-2)
    log_earn_pred[i] ~ dnorm(alpha_eth[eth[i]] + 
                    beta_height[eth[i]]*(height[i] - mean(height)), 
                    sigma^-2)
  }
  # Priors
  for(j in 1:N_eth) {
    alpha_eth[j] ~ dnorm(mu_eth, sigma_eth^-2)
    beta_height[j] ~ dnorm(mu_beta_height, sigma_height^-2)
  }
  mu_beta_height ~ dnorm(0, 0.1^-2)
  mu_eth ~ dnorm(11, 2^-2)
  sigma_eth ~ dt(0, 5^-2, 1)T(0,)
  sigma_height ~ dt(0, 1, 1)T(0,)
  sigma ~ dt(0, 5^-2, 1)T(0,)
}
'
jags_run = jags(data = list(N = nrow(dat), 
                            N_eth = length(unique(dat$eth)),
                            log_earn = log(dat$earn),
                            height = dat$height_cm,
                            eth = dat$eth),
                parameters.to.save = c('log_earn_pred'),
                model.file = textConnection(jags_code))
```
```{r, echo = FALSE, fig.height = 5}
plot(log(dat$earn), jags_run$BUGSoutput$mean$log_earn_pred, xlab = 'True log(earnings)', ylab = 'Predicted log(earnings)')
abline(a = 0, b = 1, col = 'red')
```

## Introducing age

- Let's fit an even more complicated model with intercepts and slopes varying by ethnicity and age group

- Age is divided up into three groups 1: 18-34, 2: 35-49, and 3: 50-64

- We want to know whether the degree to which height affects earnings  for different ethnic/age group combinations

## JAGS model

\tiny
```{r}
jags_code = '
model{
  # Likelihood
  for(i in 1:N) {
    log_earn[i] ~ dnorm(alpha[eth[i],age_grp[i]] +
                          beta[eth[i],age_grp[i]]*(height[i] - mean(height)), 
                    sigma^-2)
  }
  # Priors
  for(j in 1:N_eth) {
    for(k in 1:N_age_grp) {
      alpha[j,k] ~ dnorm(mu_alpha, sigma_alpha^-2)
      beta[j,k] ~ dnorm(mu_beta, sigma_beta^-2)
    }
  }
  mu_alpha ~ dnorm(11, 2^-2)
  mu_beta ~ dnorm(0, 0.1^-2)
  sigma_alpha ~ dt(0,5^-2,1)T(0,)
  sigma_beta ~ dt(0,1,1)T(0,)
  sigma ~ dt(0,5^-2,1)T(0,)
}
'
```

## Model output

```{r, include = FALSE}
jags_run = jags(data = list(N = nrow(dat), 
                            N_eth = length(unique(dat$eth)),
                            N_age_grp = length(unique(dat$age)),
                            log_earn = log(dat$earn),
                            height = dat$height_cm,
                            eth = dat$eth,
                            age_grp = dat$age),
                parameters.to.save = c('alpha',
                                       'beta'),
                model.file = textConnection(jags_code))
```
```{r, echo = FALSE, fig.height = 5}
pars = jags_run$BUGSoutput$mean
age_grp_names = c('18-34','35-49','50-64')
eth_names = c('Blacks','Hispanics','Whites','Others')

par(mfrow=c(3,4))
for(j in 1:3) {
  for(i in 1:4) {
    curr_dat = subset(dat, dat$eth == i & dat$age == j)
    plot(curr_dat$height_cm, log(curr_dat$earn), main = paste(eth_names[i], age_grp_names[j]), ylab = 'log(earnings)', xlab = 'Height (cm)')
    lines(dat$height_cm, pars$alpha[i,j] + pars$beta[i,j]*(dat$height_cm - mean (dat$height_cm)), col = i)    
  }
}
par(mfrow=c(1,1))
```

## More about this model

- So we now have varying effects - we should also plot the uncertainties in these lines (see practical)

```{r, include = FALSE}
jags_summ = jags_run$BUGSoutput$summary
DIC = round(jags_run$BUGSoutput$DIC,2)
pD = round(jags_run$BUGSoutput$pD,2)
```

- The DIC here is now DIC now $`r format(DIC,scientific=FALSE)`$ with $`r format(pD,scientific=FALSE)`$ effective parameters - a big drop!


## Missing and unbalanced data

- There are many definitions of what 'unbalanced' data means in statistics. Usually we mean that there are different numbers of observations in each group. Our format of writing e.g. $y_i \sim N(\alpha_{\mbox{eth}_i} + \beta_{\mbox{eth}_i} x_i, \sigma^2)$ allows us to deal with unbalanced data naturally

- Usually the smaller the sample size of the group the more uncertain the posterior distribution will be

- But what if we have some missing data? There are different types, and some need to be more carefully treated than others

## Different types of missing data

- There are many different types of missing data:

    - Missing response variables
    - Missing covariates
    - Missingness that occurs completely at random
    - Missingness that occurs as a consequence of the experiment or the data
    
- The first three are all very easy to deal with in JAGS (less so in Stan). The last one is much harder, and not something we will go into in any detail. It requires building a separate model for the missingness process

## The simple way of dealing with missing data in JAGS

- In JAGS it is absolutely trivial to to deal with missingness in the response variable. You simply fill in the missing values with `NA`
- JAGS then treats them as parameters to be estimated. You can 'watch' them in the normal way or just ignore them. You thus have the option of getting a posterior distribution of the missing data points
- Suppose we shoved in some NA values into our data
```{r}
dat2 = dat
dat2$earn[c(177, 763, 771)] = NA
```

## Running the model with missingness

\tiny
```{r, include = FALSE}
jags_run = jags(data = list(N = nrow(dat2), 
                            N_eth = length(unique(dat2$eth)),
                            N_age_grp = length(unique(dat2$age)),
                            log_earn = log(dat2$earn),
                            height = dat2$height_cm,
                            eth = dat2$eth,
                            age_grp = dat2$age,
                            k_beta = 2,
                            R_beta = diag(2)),
                parameters.to.save = c('log_earn[177]','log_earn[763]','log_earn[771]'),
                model.file = textConnection(jags_code))
```
```{r}
print(jags_run)
```

## More complex varieties of missing data

- If you have missing covariates or joint missing covariates/response variables, you can include these too
- The only extra issue is that you need to give JAGS a prior distribution for the missing covariate values which can make the code a bit fiddlier
- If the response variable (e.g. log earnings) exists but the covariate value is missing, then you are asking JAGS to perform an _inverse regression_
- In Stan missing data is fiddlier to incorporate as you have to separate out the parameters (i.e. missing data) from the observed data

## Summary

- We have seen how to create some rich multi-layer models
- We have gone through quite a detailed example
- We have discovered how to deal with missing and unbalanced data sets
